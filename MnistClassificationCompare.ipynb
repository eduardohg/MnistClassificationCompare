{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INTRODUÇÃO**\n",
        "\n",
        "Este é um experimento que tem como objetivo comparar duas bibliotecas de Deep Learning: TensorFlow(Keras) e PyTorch.\n",
        "\n",
        "O objetivo é construir uma CNN(Convolutional Neural Network) nas duas bibliotecas, com a mesma arquitetura e os mesmos hiperparâmetros\n",
        "\n",
        "O dataset escolhido foi o MNIST, este conjunto de dados é uma coleção de dígitos manuscritos, concebida para treinar e testar sistemas de classificação de imagens."
      ],
      "metadata": {
        "id": "MWYpsQv1Uosw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METODOLOGIA**\n",
        "\n",
        "A arquitetura da CNN (Convolutional Neural Network) é projetada para classificação de imagens em 10 classes. Abaixo está um resumo detalhado da arquitetura:\n",
        "\n",
        "1.   Camada de Entrada\n",
        "\n",
        "    *   A rede espera imagens de entrada com tamanho 28x28 pixels e 1 canal, o que sugere que as imagens são em escala de cinza (grayscale).\n",
        "\n",
        "\n",
        "2.   Camadas Convolucionais e de Pooling:\n",
        "\n",
        "    *   1ª Camada Convolucional:\n",
        "      * Filtros: 28\n",
        "      * Kernel Size: (3, 3)\n",
        "      * Função de Ativação: ReLU\n",
        "      * Esta camada aplica 28 filtros convolucionais com um kernel de tamanho 3x3 na imagem de entrada. A função de ativação ReLU é usada para introduzir não-linearidade.\n",
        "\n",
        "    *   1ª Camada de Pooling:\n",
        "      * Pool Size: (2, 2)\n",
        "      * Esta camada reduz a dimensionalidade da imagem ao aplicar uma operação de pooling com uma janela de 2x2, o que reduz pela metade a resolução espacial.\n",
        "\n",
        "    * 2ª Camada Convolucional:\n",
        "      * Filtros: 64\n",
        "      * Kernel Size: (3, 3)\n",
        "      * Função de Ativação: ReLU\n",
        "      * Outra camada convolucional que aumenta a profundidade para 64 filtros, mantendo o kernel 3x3.\n",
        "\n",
        "    * 2ª Camada de Pooling:\n",
        "      * Pool Size: (2, 2)\n",
        "      * Outra operação de pooling para reduzir a resolução espacial, mantendo as características mais importantes.\n",
        "\n",
        "    * 3ª Camada Convolucional:\n",
        "      * Filtros: 64\n",
        "      * Kernel Size: (3, 3)\n",
        "      * Função de Ativação: ReLU\n",
        "      * Mais uma camada convolucional com 64 filtros, que continua a extração de características mais profundas da imagem.\n",
        "      \n",
        "3. Camadas Densas:\n",
        "\n",
        "    * Camada Flatten:\n",
        "      * A saída das camadas convolucionais (que é uma matriz tridimensional) é achatada em um vetor unidimensional para ser usada como entrada para as camadas densas.\n",
        "\n",
        "    * 1ª Camada Densa:\n",
        "      * Neurônios: 64\n",
        "      * Função de Ativação: ReLU\n",
        "      * Esta camada totalmente conectada (fully connected) aprende combinações não-lineares das características extraídas pelas camadas convolucionais.\n",
        "\n",
        "    * 2ª Camada Densa(Saída):\n",
        "      * Neurônios: 10\n",
        "      * Esta camada final produz 10 valores, um para cada classe, sem aplicar uma função de ativação porque a perda usada (SparseCategoricalCrossentropy) será aplicada diretamente aos logits.\n",
        "\n",
        "4. Compilação do Modelo:\n",
        "\n",
        "    * Otimizador: Adam com uma taxa de aprendizado de 0,001.\n",
        "    * Função de Perda: SparseCategoricalCrossentropy (usada para tarefas de classificação onde as classes são representadas por rótulos inteiros).\n"
      ],
      "metadata": {
        "id": "pXaySdsrWBzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPLEMENTAÇÃO CNN TENSORFLOW**"
      ],
      "metadata": {
        "id": "xlHVmLn_OHfF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS89inLe4fTB"
      },
      "outputs": [],
      "source": [
        "## Bibliotecas necessárias\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import models, layers\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "Y3a7rUXAnWfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém o dataset\n",
        "(x_train_original,y_train),(x_test_original, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train_original, axis=-1)\n",
        "x_test = np.expand_dims(x_test_original, axis=-1)"
      ],
      "metadata": {
        "id": "pJChkSBu4lUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "dCa0umNy9L6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constrói o modelo CNN\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(28, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ39cuHv72sC",
        "outputId": "a1ae5178-3f86-41c2-b318-995129839b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compila o modelo\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "775m0tnn8LxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treina o modelo\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64,\n",
        "                    validation_data=(x_test, y_test))\n",
        "end_time = time.time()\n",
        "keras_time = end_time - start_time\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_39tEli8Ol2",
        "outputId": "133ab871-bf20-4467-b245-558598e5126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 60ms/step - accuracy: 0.8732 - loss: 0.4227 - val_accuracy: 0.9802 - val_loss: 0.0593\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 57ms/step - accuracy: 0.9834 - loss: 0.0524 - val_accuracy: 0.9862 - val_loss: 0.0467\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - accuracy: 0.9889 - loss: 0.0356 - val_accuracy: 0.9820 - val_loss: 0.0562\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.9921 - loss: 0.0271 - val_accuracy: 0.9786 - val_loss: 0.0760\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.9938 - loss: 0.0219 - val_accuracy: 0.9867 - val_loss: 0.0440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPLEMENTAÇÃO CNN PYTORCH**"
      ],
      "metadata": {
        "id": "r_9FdY2GbYQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "xhdJFhmQWJmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "8jMKG1AMnakL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformações para normalizar o dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "vOTtmZDhWhGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset MNIST\n",
        "train_images = torch.tensor(x_train_original, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "train_labels = torch.tensor(y_train, dtype=torch.long)\n",
        "test_images = torch.tensor(x_test_original, dtype=torch.float32).unsqueeze(1) / 255.0\n",
        "test_labels = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Criar datasets e dataloaders\n",
        "train_dataset = TensorDataset(train_images, train_labels)\n",
        "test_dataset = TensorDataset(test_images, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "cfbi1661Sn3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataLoader para facilitar o treinamento\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "EeKv5e6ySpA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura da CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # A camada Conv2D inicial de TensorFlow tinha 28 filtros\n",
        "        self.conv1 = nn.Conv2d(1, 28, kernel_size=3, stride=1, padding=0)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(28, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 3 * 3, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 64 * 3 * 3)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zNTNlW66Ss1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar o modelo e definir a função de perda e otimizador\n",
        "model_pytorch = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OUgV2HvCTHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo\n",
        "start_time = time.time()\n",
        "for epoch in range(5):  # 5 épocas\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_pytorch(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}: Loss {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "end_time = time.time()\n",
        "pytorch_time = end_time - start_time\n",
        "\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzEWWmlUTKnE",
        "outputId": "a1cc332f-f850-4800-ee6b-758b7dac0ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss 0.973\n",
            "Epoch 1, Batch 200: Loss 0.321\n",
            "Epoch 1, Batch 300: Loss 0.220\n",
            "Epoch 1, Batch 400: Loss 0.146\n",
            "Epoch 1, Batch 500: Loss 0.126\n",
            "Epoch 1, Batch 600: Loss 0.099\n",
            "Epoch 1, Batch 700: Loss 0.098\n",
            "Epoch 1, Batch 800: Loss 0.087\n",
            "Epoch 1, Batch 900: Loss 0.078\n",
            "Epoch 2, Batch 100: Loss 0.073\n",
            "Epoch 2, Batch 200: Loss 0.059\n",
            "Epoch 2, Batch 300: Loss 0.064\n",
            "Epoch 2, Batch 400: Loss 0.067\n",
            "Epoch 2, Batch 500: Loss 0.061\n",
            "Epoch 2, Batch 600: Loss 0.057\n",
            "Epoch 2, Batch 700: Loss 0.050\n",
            "Epoch 2, Batch 800: Loss 0.054\n",
            "Epoch 2, Batch 900: Loss 0.054\n",
            "Epoch 3, Batch 100: Loss 0.044\n",
            "Epoch 3, Batch 200: Loss 0.043\n",
            "Epoch 3, Batch 300: Loss 0.052\n",
            "Epoch 3, Batch 400: Loss 0.047\n",
            "Epoch 3, Batch 500: Loss 0.036\n",
            "Epoch 3, Batch 600: Loss 0.047\n",
            "Epoch 3, Batch 700: Loss 0.035\n",
            "Epoch 3, Batch 800: Loss 0.043\n",
            "Epoch 3, Batch 900: Loss 0.040\n",
            "Epoch 4, Batch 100: Loss 0.037\n",
            "Epoch 4, Batch 200: Loss 0.032\n",
            "Epoch 4, Batch 300: Loss 0.031\n",
            "Epoch 4, Batch 400: Loss 0.037\n",
            "Epoch 4, Batch 500: Loss 0.032\n",
            "Epoch 4, Batch 600: Loss 0.039\n",
            "Epoch 4, Batch 700: Loss 0.031\n",
            "Epoch 4, Batch 800: Loss 0.028\n",
            "Epoch 4, Batch 900: Loss 0.027\n",
            "Epoch 5, Batch 100: Loss 0.026\n",
            "Epoch 5, Batch 200: Loss 0.030\n",
            "Epoch 5, Batch 300: Loss 0.028\n",
            "Epoch 5, Batch 400: Loss 0.022\n",
            "Epoch 5, Batch 500: Loss 0.033\n",
            "Epoch 5, Batch 600: Loss 0.027\n",
            "Epoch 5, Batch 700: Loss 0.021\n",
            "Epoch 5, Batch 800: Loss 0.031\n",
            "Epoch 5, Batch 900: Loss 0.022\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTADOS**"
      ],
      "metadata": {
        "id": "67pJN1aJuEf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEMPO E MÉTRICAS TENSORFLOW"
      ],
      "metadata": {
        "id": "e6MnItL6uZ5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tempo de treinamento Tensorflow: {keras_time:.2f} segundos\")\n",
        "# Avalia o modelo em acurácia\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'\\nTest accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlPrYK3mue5Q",
        "outputId": "57119dd0-719b-4621-ae5c-7bfa58b532f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treinamento Tensorflow: 357.11 segundos\n",
            "313/313 - 4s - 14ms/step - accuracy: 0.9867 - loss: 0.0440\n",
            "\n",
            "Test accuracy: 0.9866999983787537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz as predições\n",
        "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
        "\n",
        "# Avalia o modelo usando matriz de confusão e classification report\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Hag6N3ui4q",
        "outputId": "7cc44704-0f67-461f-b31b-877fad4d053f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
            "Confusion Matrix:\n",
            "[[ 969    0    2    0    0    2    4    3    0    0]\n",
            " [   0 1123    5    3    0    0    0    2    2    0]\n",
            " [   0    0 1022    2    0    0    0    7    1    0]\n",
            " [   0    0    1 1007    0    2    0    0    0    0]\n",
            " [   0    0    5    0  959    0    0    0    7   11]\n",
            " [   0    0    1    6    0  884    1    0    0    0]\n",
            " [   2    3    1    0    3    7  938    0    4    0]\n",
            " [   0    1    3    1    0    0    0 1018    0    5]\n",
            " [   0    0    1    5    0    2    0    3  962    1]\n",
            " [   1    0    1    4    0    4    0    3   11  985]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       980\n",
            "           1       1.00      0.99      0.99      1135\n",
            "           2       0.98      0.99      0.99      1032\n",
            "           3       0.98      1.00      0.99      1010\n",
            "           4       1.00      0.98      0.99       982\n",
            "           5       0.98      0.99      0.99       892\n",
            "           6       0.99      0.98      0.99       958\n",
            "           7       0.98      0.99      0.99      1028\n",
            "           8       0.97      0.99      0.98       974\n",
            "           9       0.98      0.98      0.98      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEMPO E MÉTRICAS PYTORCH"
      ],
      "metadata": {
        "id": "Rfnws1apuupO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tempo de treinamento PyTorch: {pytorch_time:.2f} segundos\")\n",
        "\n",
        "# Testar o modelo\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model_pytorch(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJVwadGnuy5o",
        "outputId": "3878234e-4c1c-44c9-9cc4-bdf76882b7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de treinamento PyTorch: 248.17 segundos\n",
            "Accuracy of the network on the 10000 test images: 98.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_predictions))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGovFFeau3bB",
        "outputId": "5eb1d555-fd0a-4395-eb88-077a8307b8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 978    0    0    0    0    0    1    1    0    0]\n",
            " [   0 1133    1    1    0    0    0    0    0    0]\n",
            " [   1    1 1025    0    2    0    0    3    0    0]\n",
            " [   0    0    2 1004    0    2    0    1    1    0]\n",
            " [   0    0    0    0  976    0    0    0    0    6]\n",
            " [   2    0    0   14    0  867    2    1    2    4]\n",
            " [   3    2    0    1    3    2  946    0    1    0]\n",
            " [   0    3    4    0    1    0    0 1016    1    3]\n",
            " [   3    1    4    2    1    0    0    2  956    5]\n",
            " [   1    1    1    0    3    3    0    8    0  992]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       980\n",
            "           1       0.99      1.00      1.00      1135\n",
            "           2       0.99      0.99      0.99      1032\n",
            "           3       0.98      0.99      0.99      1010\n",
            "           4       0.99      0.99      0.99       982\n",
            "           5       0.99      0.97      0.98       892\n",
            "           6       1.00      0.99      0.99       958\n",
            "           7       0.98      0.99      0.99      1028\n",
            "           8       0.99      0.98      0.99       974\n",
            "           9       0.98      0.98      0.98      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANÁLISE COMPARATIVA**\n",
        "\n",
        "As duas soluções trouxeram resultados divergentes em tempo e muito semelhantes em métricas\n",
        "\n",
        "* Tempo de treinamento Tensorflow: 357.11 segundos\n",
        "* Tempo de treinamento PyTorch: 248.17 segundos\n",
        "\n",
        "* Acurácia Tensorfow: 98.67%\n",
        "* Acurácia PyTorch: 98.93%\n",
        "\n",
        "A matriz de confusão com as métricas precision f1-score e recall foram semelhantes em todas as classes"
      ],
      "metadata": {
        "id": "3kIy8e5CzwGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSÃO**\n",
        "\n",
        "1. Tempo de Treinamento\n",
        "\n",
        "  * O PyTorch treinou o modelo significativamente mais rápido (cerca de 30% mais rápido) em comparação ao TensorFlow. Se o tempo de treinamento é um fator crítico, PyTorch pode ser uma melhor escolha.\n",
        "\n",
        "2. Desempenho do Modelo\n",
        "\n",
        "  * Ambas as implementações fornecem resultados de acurácia e métricas muito semelhantes, com uma ligeira vantagem para o PyTorch. Essa diferença pode ser considerada insignificante para a maioria das aplicações, indicando que ambos os frameworks são capazes de treinar modelos altamente eficazes.\n",
        "\n",
        "3. Facilidade de Uso e Flexibilidade:\n",
        "\n",
        "  * TensorFlow, com Keras, oferece uma interface de alto nível que pode ser mais intuitiva e rápida para o desenvolvimento, enquanto PyTorch é frequentemente preferido para experimentação e pesquisa devido à sua flexibilidade e integração com o Python nativo.\n",
        "\n",
        "  Ambos os frameworks são altamente capazes de produzir modelos precisos e eficazes. A escolha entre TensorFlow e PyTorch pode depender mais de fatores como tempo de treinamento, preferências de desenvolvimento, e requisitos de produção. Para tarefas que exigem treinamento rápido, PyTorch pode ser preferível, enquanto TensorFlow pode ser a escolha para sistemas de produção robustos com suporte para deployment escalável."
      ],
      "metadata": {
        "id": "MruEunJx1cRk"
      }
    }
  ]
}